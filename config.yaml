project:
  seed: 42

dataset:
  source: huggingface  # huggingface | kaggle (kaggle is a stub unless you add credentials)
  hf_name: cnn_dailymail
  hf_config: "3.0.0"

filtering:
  min_article_tokens: 50
  max_article_tokens: 1024
  min_summary_tokens: 10
  max_summary_tokens: 256

preprocessing:
  lowercase: false
  sentence_dropout_prob: 0.0

tokenization:
  model_name: t5-small  # or t5-base / facebook/bart-large-cnn (GPU recommended)
  max_input_length: 512
  max_summary_length: 128

training:
  output_dir: ai_summarizer/models/checkpoints
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  learning_rate: 3.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  fp16: false
  bf16: false
  evaluation_strategy: epoch
  save_strategy: epoch
  load_best_model_at_end: true
  metric_for_best_model: rougeL
  greater_is_better: true
  early_stopping_patience: 2

evaluation:
  # BERTScore defaults to roberta-large (~1.4GB). Use smaller default locally.
  bertscore_model_type: distilroberta-base
  bertscore_lang: en

inference:
  device: auto  # auto | cpu | cuda
  warmup: true
  default_decoding:
    strategy: beam  # greedy | beam | topk | topp
    num_beams: 4
    max_length: 120
    min_length: 30
    temperature: 1.0
    top_k: 50
    top_p: 0.95
    repetition_penalty: 1.2

logging:
  log_dir: ai_summarizer/logs
  requests_jsonl: requests.jsonl
  usage_json: usage.json
  last_eval_json: last_eval.json
